What is Hadoop?

- provides distributed processing of data across a cluster of nodes

It has two components:
- MapReduce: a programming model that breaks down problems into discrete units that can be processed in parallel.
- HDFS: a distributed file system that replicates data across the cluster for reliability and availability 
(a minimum of a 1:3 ratio)

All about failure:
- Partial failure support: failure of a node results in graceful degregation.
- Data recoverability: If a node fails workload is handled by the surviving nodes.
- Component recoverability: If a node returns it should be able to rejoin the cluster.
- Consistency: node failures should not affect the outcome of the job.
- Scalability: adding load should result in graceful decline of performance.

3 phases to MapReduce: Map, Shuffle-sort, Reduce
- Map: All nodes operates on one portion of the data set 
  * For example, count the number of times a word appears in a document 
- Shuffle-sort: An intermediate phase to organize the data from the Map phase to the Reducer phase
- Reduce: "One" or more nodes consolidates the data generated by the map tasks and returns the result.
  * For example, sum the number of times the word appears in the document.
  
"Hello World" of Hadoop 
========================================================================================
Count and sum the number of times each word appears in a document.
A document consisting of "the cat sat on the hat" exists on the HDFS.
Start:
- Mappers run in parallel across the cluster and finds each word.
- Map phase results in the following key/values:
the: [1]
cat: [1]
sat: [1]
on:  [1] 
the: [1]
hat: [1]
- This output goes to the shuffle-sort phase.
- Shuffle-sort phase results in the following key/values:
cat: [1]
hat: [1]
on:  [1]
sat: [1]
the: [1,1]
-  This output goes to the Reducer and returns the final result of key/values:
the: 2
cat: 1
hat: 1
on:  1
sat: 1

-All mapper worked on a small portion of the data (typically a single block).
-Shuffle-sort organized the data 
-One or more reducers produced the final result (get to that in a minute)

Concept of "Bring the CPU to the data, not the other way around".  
A Shared-nothing architecture (nodes don't communicate).  Everything processed in parallel.
Based on Google technology and never patented.

Hadoop is good at things relational databases are bad a
RDBMS good for single lookup, bad for table scan
Hadoop good for table scan, but not a single query mechanism

- Great dealing with large amounts of Volume, Velocity and Variety of data.  
- Batch Processing, not individual queries is what its good for.
- Not a panacea. But new technologies like Hbase (distributed data score based on HDFS)
are resulting in near real-time performance of single lookups.

What's it good for besides counting words?
===========================================
Graph processing (A graph is a set of nodes(vertices) connected by lines(edges)"
Different types of graphs: directed, undirected, cyclic, acyclic, weighted (some more expensive to traverse)
- Find the shortest path thru a graph: routing IP traffic, driving fdirections
- Spanning Trees: laying fiber, UPS trucks
- Maximum flow: airline scheduling
- Finding critical nodes which can break a graph into disjoint components: 
  - identifying opinion leaders, epidemic carriers, terrorist cells
- Breath-first for MapReduce works better with parallellism than depth-first (dijkstras algorithm 
which is single threaded) - Not as efficent due to parallelism, plus needs 1 reducer to properly sort the data

- Again, not a panacea

Weeds
=====

How the components work:
A master program called the Job Tracker accepts a job and allocates work to nodes 
(slaves running thier own Task Tracker) that work on blocks stored locally on that node.
Many Nodes working together on thier own part of the data set.

- NameNode: manages the HDFS
- Secondary NameNode: performs housekeeping for NameNode
- Job Tracker: Allocates work to nodes (slaves) across the cluster
- DataNode: manages the data on a slave node
- Task Tracker: manages the job executing on it's slave node.

What if there is a problem with the slave nodes? Hadoop has a feature called "speculative execution" that can 
replicate to other slave nodes.  Whatever node finishes first notifies the Task Tracker, and kills the other task.

Applications are written at a high level, shielding the developer from the complexity of the infrastructure.

Job stages:
- Input Reader
- Map
- Partition
- Combiner
- Reduce
- Output Writer

