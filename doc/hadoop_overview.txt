What is Hadoop?
- distributed processing of data across a cluster of nodes

It has two components:
- MapReduce: a programming model that breaks down problems into discrete units that can be processed in parallel.
- HDFS: a distributed file system that replicates data across the cluster for reliability and availability 
(a minimum of a 1:3 ratio. )

All about failure:
- Partial failure support: failure of a node results in graceful degregation.
- Data recoverability: If a node fails workload is handled by the surviving nodes.
- Component recoverability: If a node returns it should be able to rejoin the cluster.
- Consistency: node failures should not affect the outcome of the job.
- Scalability: adding load should result in graceful decline of performance.

3 phases to MapReduce: Map, Shuffle-sort, Reduce
- Map: All node operates on one portion of the data set.
  For example, count the number of times a word appears in a document 
- Shuffle-sort: An intermediate phase to organize the data from the Map phase to the Reducer phase
- Reduce: One(!) or more nodes consolidates the data generated by the map tasks and returns the result.

  For example, sum the number of times the word appears in the document.
  
"Hello World" of Hadoop (WordCount)
===================================
- Map takes input: "the cat sat on the hat" and distribues it across the cluster
- Map phase outputs:
the: [1]
cat: [1]
sat: [1]
on:  [1] 
the: [1]
hat: [1]
- Output of map data goes to input of shuffle-sort 
- Shuffle-sort phase outputs (to reducer)
cat: [1]
hat: [1]
on:  [1]
sat: [1]
the: [1,1]
- Reducer takes output of shuffle-and-sort and returns final result:
the: 2
cat: 1
hat: 1
on:  1
sat: 1

Each map tasks works on a small portion of the data (typically a single block). 
Concept of "Bring the CPU to the data, not the other way around".  
A Shared-nothing architecture (nodes don't communicate).  Everything processes in parallel*

How it works:
A master program called the Job Tracker accepts a job and allocates work to nodes 
(slaves running thier own Task Tracker) that work on blocks stored locally on that node.
Many Nodes working together on thier own part of the data set.

- NameNode: manages the HDFS
- Secondary NameNode: performs housekeeping for NameNode
- Job Tracker: Allocates work to nodes (slaves) across the cluster
- DataNode: manages the data on a slave node
- Task Tracker: manages the job executing on it's slave node.

What if there is a problem with the slave nodes? Hadoop has a feature called "speculative execution" that can 
replicate to other slave nodes.  Whatever node finishes first notifies the Task Tracker, and kills the other task.

Hadoop is good at things relational databases are bad a
RDBMS good for single lookup, bad for table scan
Hadoop good for table scan, but not a single query mechanism

- Deals with large amounts of Volume, Velocity, Variety of data.  
- Batch Processing

Applications are written at a high level, shielding the developer from the complexity of the infrastructure.

Weeds:
------
Job stages:
- Input Reader
- Map
- Partition
- Combiner
- Reduce
- Output Writer

